{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipit099/Deep_Learning-Colab/blob/main/Titanic_Machine_Learning_from_Disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GiJNidoA5VU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eay_k4LEBbfe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/train.csv')\n",
        "test_data  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/test.csv')\n"
      ],
      "metadata": {
        "id": "F_fkmunpCtV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "GciqgctiCt2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "vw8DptstCwp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "KxV7b3z8Cxt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "id": "w2sDzf0uC4F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = train_data.select_dtypes(include=['object']).columns\n",
        "print(object_columns)"
      ],
      "metadata": {
        "id": "4nr8tk3nDJh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "id": "OrIZhAT_CzcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "itGbJZDRInUw"
      },
      "outputs": [],
      "source": [
        "# # Set the figure size\n",
        "# plt.figure(figsize=(10, 6))\n",
        "\n",
        "# # Create a scatter plot\n",
        "# sns.scatterplot(x='Fare', y='Survived', data=train_data, hue='Survived', palette={0: 'red', 1: 'green'})\n",
        "\n",
        "# # Set the title and labels\n",
        "# plt.title('Fare vs Survived (Scatter Plot)')\n",
        "# plt.xlabel('Fare')\n",
        "# plt.ylabel('Survived (0 = No, 1 = Yes)')\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Cabin'], axis=1)"
      ],
      "metadata": {
        "id": "2qyX_b7crYoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_data))"
      ],
      "metadata": {
        "id": "G73ColSUq7KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data Loading and Preprocessing:\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "train_data['Age'] = imputer.fit_transform(train_data[['Age']])\n",
        "\n",
        "# #handle categorical data\n",
        "label_encoders = {}\n",
        "for column in ['Sex', 'Embarked']:\n",
        "    le = LabelEncoder()\n",
        "    train_data[column] = le.fit_transform(train_data[column])\n",
        "    label_encoders[column] = le\n"
      ],
      "metadata": {
        "id": "rAr1-O5e2qz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tt = train_data.drop(['PassengerId', 'Survived'], axis=1)\n",
        "y_tt = train_data['Survived']\n",
        "\n"
      ],
      "metadata": {
        "id": "h9iJay_noITA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Normalize the data\n",
        "# scaler = StandardScaler()\n",
        "# X = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "QmeQrRLHm0Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to numpy arrays...ONCE possible only\n",
        "X= X_tt.values\n",
        "y= y_tt.values"
      ],
      "metadata": {
        "id": "kbR20pxYpwAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "6equ_q1ynOkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_tt))\n",
        "print(type(X))\n",
        "print(type(X_train))\n",
        "\n",
        "# <class 'pandas.core.frame.DataFrame'>\n",
        "# <class 'numpy.ndarray'>\n",
        "# <class 'numpy.ndarray'>"
      ],
      "metadata": {
        "id": "hUdXWz-5rx_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Convert to PyTorch tensors\n",
        "# Convert X features to float tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "\n",
        "# Convert y labels to long tensors\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Verify lengths\n",
        "len(X_train_tensor), len(X_test_tensor), len(y_train_tensor), len(y_test_tensor)"
      ],
      "metadata": {
        "id": "5lhRO0K5qstv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaded into tensor model.. otherwise u cant use tensor functions\n",
        "# Create TensorDataset objects\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "len(train_loader), len(test_loader)"
      ],
      "metadata": {
        "id": "ahkcEPeHoQLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model Definition\n",
        "\n",
        "\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TitanicModel, self).__init__()\n",
        "        self.layer_1 = nn.Linear(X_train_tensor.shape[1], 128)\n",
        "        self.layer_2 = nn.Linear(128, 64)\n",
        "        self.layer_3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()  # using ReLU activation function\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer_1(x))\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "jxiGMLP6_k6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TitanicModel()\n",
        "criterion = nn.BCEWithLogitsLoss()      # loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # using Adam as optimizer and learning rate\n"
      ],
      "metadata": {
        "id": "ilyZN0OnzFsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = (y_true == y_pred).sum().item()\n",
        "    return (correct / len(y_true)) * 100"
      ],
      "metadata": {
        "id": "vzdljUF5So_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "# Initialize lists to store losses and accuracies\n",
        "epoch_count = []\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "train_acc_values = []\n",
        "test_acc_values = []\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(32)\n",
        "epochs = 500\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    total_loss = 0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # 1. Forward pass\n",
        "        y_logits = model(batch_X).squeeze()\n",
        "        y_pred = torch.round(torch.sigmoid(y_logits))  # Convert logits to prediction probabilities, then to prediction labels\n",
        "\n",
        "        # 2. Calculate loss and accuracy\n",
        "        loss = criterion(y_logits, batch_y.float())\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Accuracy calculation\n",
        "        acc = accuracy_fn(y_true=batch_y, y_pred=y_pred)\n",
        "        correct_train += (y_pred.squeeze() == batch_y).sum().item()\n",
        "        total_train += len(batch_y)\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_acc = (correct_train / total_train) * 100\n",
        "\n",
        "    # Evaluate on the test set after each epoch\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "\n",
        "        test_logits = model(X_test_tensor).squeeze()\n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))  # Convert logits to prediction probabilities, then to prediction labels\n",
        "\n",
        "        # Count correct predictions and total predictions\n",
        "        correct_test = (test_pred.squeeze() == y_test_tensor).sum().item()\n",
        "        total_test = len(y_test_tensor)\n",
        "\n",
        "        test_loss = criterion(test_logits, y_test_tensor.float())\n",
        "        test_acc = (correct_test / total_test) * 100\n",
        "\n",
        "    # Store values for plotting\n",
        "    epoch_count.append(epoch)\n",
        "    train_loss_values.append(avg_train_loss)\n",
        "    test_loss_values.append(test_loss.item())\n",
        "    train_acc_values.append(train_acc)\n",
        "    test_acc_values.append(test_acc)\n",
        "\n",
        "    # Print out the progress every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch} | Train Loss: {avg_train_loss:.5f}, Train Accuracy: {train_acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Epoch: {epoch+1} | Train Loss: {avg_train_loss:.5f}, Train Accuracy: {train_acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n",
        "print(f\"Total Correct Test Predictions: {correct_test}\")\n",
        "print(f\"Total Test Samples: {total_test}\")\n"
      ],
      "metadata": {
        "id": "rxeLJrqpK-7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and test loss\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch_count, train_loss_values, label='Train Loss')\n",
        "plt.plot(epoch_count, test_loss_values, label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss over Epochs')\n",
        "# Plot the training and test accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch_count, train_acc_values, label='Train Accuracy')\n",
        "plt.plot(epoch_count, test_acc_values, label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.title('Accuracy over Epochs')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DKGLRwlZOH0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.manual_seed(42)\n",
        "# epochs = 200\n",
        "# epoch_count = []\n",
        "# train_loss_values = []\n",
        "# test_loss_values = []\n",
        "\n",
        "# # Training loop\n",
        "# for epoch in range(epochs):\n",
        "#     model.train()\n",
        "#     for batch_X, batch_y in train_loader:\n",
        "#         # Forward pass\n",
        "#         y_pred = model(batch_X).squeeze()  # Get predicted results and remove extra dimensions\n",
        "\n",
        "#         # Measure the loss/error\n",
        "#         loss = criterion(y_pred, batch_y.float())  # Convert batch_y to float for BCEWithLogitsLoss\n",
        "\n",
        "#         # Backward pass and optimization\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     # Evaluate on the test set after each epoch\n",
        "#     model.eval()\n",
        "#     with torch.inference_mode():\n",
        "#         # Use torch.round and torch.sigmoid to get predicted classes\n",
        "#         y_preds = torch.round(torch.sigmoid(model(X_test_tensor))).squeeze()\n",
        "\n",
        "#         # Calculate test loss\n",
        "#         test_loss = criterion(y_preds, y_test_tensor.float())\n",
        "\n",
        "#     if epoch % 10 == 0:\n",
        "#         # Record losses\n",
        "#         epoch_count.append(epoch)\n",
        "#         train_loss_values.append(loss.item())\n",
        "#         test_loss_values.append(test_loss.item())\n",
        "\n",
        "#         print(f'Epoch: {epoch} | Loss: {loss.item():.4f} | Test Loss: {test_loss.item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "fTWe2-Cb_4dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "\n",
        "test_data = test_data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Cabin'], axis=1)\n",
        "test_data['Age'] = imputer.transform(test_data[['Age']])\n",
        "\n",
        "for column in ['Sex', 'Embarked']:\n",
        "    test_data[column] = label_encoders[column].transform(test_data[column])\n",
        "\n"
      ],
      "metadata": {
        "id": "_YgCdcw9_zNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dont remove rows from testdata\n",
        "test_data.isnull().sum()"
      ],
      "metadata": {
        "id": "9a6gjS-ZAIBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "id": "fUoFWiz4BHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "LmUOxKGpATiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data for prediction\n",
        "# Drop the 'PassengerId' column as it's not a feature\n",
        "X_test = test_data.drop(['PassengerId'], axis=1)\n",
        "\n",
        "# Save the 'PassengerId' column to use later for the output file\n",
        "PassengerId = test_data['PassengerId']\n",
        "\n",
        "# Apply the same scaler used on the training data to the test data\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert the scaled test data to a PyTorch tensor\n",
        "X_test_tensor = torch.FloatTensor(X_test.values)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for prediction\n",
        "with torch.no_grad():\n",
        "    # Forward pass: compute the model output for the test data\n",
        "    outputs = model(X_test_tensor).squeeze()\n",
        "\n",
        "    # Convert logits to probabilities and then to binary predictions\n",
        "    predictions = torch.round(torch.sigmoid(outputs)).long()\n",
        "\n"
      ],
      "metadata": {
        "id": "bTTBd3b7AGJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the 'PassengerId' and the corresponding predictions\n",
        "output_df = pd.DataFrame({\n",
        "    'PassengerId': PassengerId,\n",
        "    'Survived': predictions.numpy()\n",
        "})\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/output.csv', index=False)\n",
        "\n",
        "# Print a message indicating that the output has been saved\n",
        "print('Output saved to output.csv')\n"
      ],
      "metadata": {
        "id": "2y6goJUsU_Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tgPL_xY6QQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMaahDac+XRdr1tBwg+UI98",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}