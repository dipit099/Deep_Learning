{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipit099/Deep_Learning-Colab/blob/main/Titanic_Machine_Learning_from_Disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GiJNidoA5VU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eay_k4LEBbfe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/train.csv')\n",
        "test_data  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/test.csv')\n"
      ],
      "metadata": {
        "id": "F_fkmunpCtV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "GciqgctiCt2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "vw8DptstCwp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "KxV7b3z8Cxt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "id": "w2sDzf0uC4F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = train_data.select_dtypes(include=['object']).columns\n",
        "print(object_columns)"
      ],
      "metadata": {
        "id": "4nr8tk3nDJh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "id": "OrIZhAT_CzcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "itGbJZDRInUw"
      },
      "outputs": [],
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a scatter plot\n",
        "sns.scatterplot(x='Fare', y='Survived', data=train_data, hue='Survived', palette={0: 'red', 1: 'green'})\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Fare vs Survived (Scatter Plot)')\n",
        "plt.xlabel('Fare')\n",
        "plt.ylabel('Survived (0 = No, 1 = Yes)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading and Preprocessing:\n",
        "train_data = train_data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "train_data['Age'] = imputer.fit_transform(train_data[['Age']])\n",
        "\n",
        "#handle categorical data\n",
        "label_encoders = {}\n",
        "for column in ['Sex', 'Embarked']:\n",
        "    le = LabelEncoder()\n",
        "    train_data[column] = le.fit_transform(train_data[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "X = train_data.drop(['PassengerId', 'Survived'], axis=1)\n",
        "y = train_data['Survived']\n",
        "\n",
        "# # Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.int64)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Loaded into tensor model.. otherwise u cant use tensor functions\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "rAr1-O5e2qz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TitanicModel, self).__init__()\n",
        "        self.layer_1 = nn.Linear(X_tensor.shape[1], 128)\n",
        "        self.layer_2 = nn.Linear(128, 64)\n",
        "        self.layer_3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()  #using ReLU activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer_1(x))\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n",
        "model = TitanicModel()\n",
        "criterion = nn.BCEWithLogitsLoss()      # loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # using Adam as optimizer and learning rate\n"
      ],
      "metadata": {
        "id": "jxiGMLP6_k6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "# Number of epochs to train the model\n",
        "num_epochs = 10\n",
        "\n",
        "# Loop over the dataset multiple times\n",
        "for epoch in range(num_epochs):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize the epoch loss\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    # Loop over the batches of data\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: compute the model output\n",
        "        outputs = model(batch_X).squeeze()\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, batch_y.float())\n",
        "\n",
        "        # Backward pass: compute the gradients\n",
        "        loss.backward()     # Back propagation\n",
        "\n",
        "        # Update the model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss for this batch\n",
        "        epoch_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    # Compute the average loss for this epoch\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Print the loss for this epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "4weGY86v_xks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot the training loss and accuracy\n",
        "# fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# # Plot loss\n",
        "# color = 'tab:red'\n",
        "# ax1.set_xlabel('Epoch')\n",
        "# ax1.set_ylabel('Loss', color=color)\n",
        "# ax1.plot(range(1, num_epochs + 1), epoch_losses, marker='o', linestyle='-', color=color)\n",
        "# ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# # Create a second y-axis to plot accuracy\n",
        "# ax2 = ax1.twinx()\n",
        "# color = 'tab:blue'\n",
        "# ax2.set_ylabel('Accuracy', color=color)\n",
        "# ax2.plot(range(1, num_epochs + 1), epoch_accuracies, marker='x', linestyle='--', color=color)\n",
        "# ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "# fig.tight_layout()\n",
        "# plt.title('Training Loss and Accuracy over Epochs')\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "cbNbZe__UYa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model on test data and compare\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize total loss and counters for accuracy\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Disable gradient calculation for evaluation\n",
        "    with torch.no_grad():\n",
        "        # Loop over the batches in the test loader\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            # Forward pass: compute the model output\n",
        "            outputs = model(batch_X).squeeze()\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, batch_y.float())\n",
        "\n",
        "            # Accumulate the loss for this batch\n",
        "            total_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "            # Convert logits to probabilities and then to binary predictions\n",
        "            predictions = torch.round(torch.sigmoid(outputs))\n",
        "\n",
        "            # Count correct predictions\n",
        "            correct += (predictions == batch_y).sum().item()\n",
        "\n",
        "            # Count total samples\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    # Calculate the average loss for the test set\n",
        "    average_test_loss = total_loss / len(test_loader.dataset)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct / total\n",
        "\n",
        "    # Print the average loss and accuracy\n",
        "    print(f'Average Test Loss: {average_test_loss:.4f}')\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Evaluate the model using the test data\n",
        "evaluate_model(model, test_loader, criterion)\n"
      ],
      "metadata": {
        "id": "fTWe2-Cb_4dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "\n",
        "test_data = test_data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Cabin'], axis=1)\n",
        "test_data['Age'] = imputer.transform(test_data[['Age']])\n",
        "\n",
        "for column in ['Sex', 'Embarked']:\n",
        "    test_data[column] = label_encoders[column].transform(test_data[column])\n",
        "\n"
      ],
      "metadata": {
        "id": "_YgCdcw9_zNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dont remove rows from testdata\n",
        "test_data.isnull().sum()"
      ],
      "metadata": {
        "id": "9a6gjS-ZAIBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "id": "fUoFWiz4BHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "LmUOxKGpATiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data for prediction\n",
        "# Drop the 'PassengerId' column as it's not a feature\n",
        "X_test = test_data.drop(['PassengerId'], axis=1)\n",
        "\n",
        "# Save the 'PassengerId' column to use later for the output file\n",
        "PassengerId = test_data['PassengerId']\n",
        "\n",
        "# Apply the same scaler used on the training data to the test data\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert the scaled test data to a PyTorch tensor\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for prediction\n",
        "with torch.no_grad():\n",
        "    # Forward pass: compute the model output for the test data\n",
        "    outputs = model(X_test_tensor).squeeze()\n",
        "\n",
        "    # Convert logits to probabilities and then to binary predictions\n",
        "    predictions = torch.round(torch.sigmoid(outputs)).long()\n",
        "\n"
      ],
      "metadata": {
        "id": "bTTBd3b7AGJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the 'PassengerId' and the corresponding predictions\n",
        "output_df = pd.DataFrame({\n",
        "    'PassengerId': PassengerId,\n",
        "    'Survived': predictions.numpy()\n",
        "})\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/output.csv', index=False)\n",
        "\n",
        "# Print a message indicating that the output has been saved\n",
        "print('Output saved to output.csv')\n"
      ],
      "metadata": {
        "id": "2y6goJUsU_Ok"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPvMFl5dMKFZppA3GIpJ02u",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}