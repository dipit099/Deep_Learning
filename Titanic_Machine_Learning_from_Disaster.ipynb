{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipit099/Deep_Learning-Colab/blob/main/Titanic_Machine_Learning_from_Disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GiJNidoA5VU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eay_k4LEBbfe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/train.csv')\n",
        "test_data  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/test.csv')\n"
      ],
      "metadata": {
        "id": "F_fkmunpCtV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "GciqgctiCt2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "vw8DptstCwp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "KxV7b3z8Cxt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "id": "w2sDzf0uC4F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = train_data.select_dtypes(include=['object']).columns\n",
        "print(object_columns)"
      ],
      "metadata": {
        "id": "4nr8tk3nDJh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "id": "OrIZhAT_CzcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "itGbJZDRInUw"
      },
      "outputs": [],
      "source": [
        "# # Set the figure size\n",
        "# plt.figure(figsize=(10, 6))\n",
        "\n",
        "# # Create a scatter plot\n",
        "# sns.scatterplot(x='Fare', y='Survived', data=train_data, hue='Survived', palette={0: 'red', 1: 'green'})\n",
        "\n",
        "# # Set the title and labels\n",
        "# plt.title('Fare vs Survived (Scatter Plot)')\n",
        "# plt.xlabel('Fare')\n",
        "# plt.ylabel('Survived (0 = No, 1 = Yes)')\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Cabin'], axis=1)"
      ],
      "metadata": {
        "id": "2qyX_b7crYoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_data))"
      ],
      "metadata": {
        "id": "G73ColSUq7KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Data Loading and Preprocessing:\n",
        "\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "train_data['Age'] = imputer.fit_transform(train_data[['Age']])\n",
        "\n",
        "# #handle categorical data\n",
        "label_encoders = {}\n",
        "for column in ['Sex', 'Embarked']:\n",
        "    le = LabelEncoder()\n",
        "    train_data[column] = le.fit_transform(train_data[column])\n",
        "    label_encoders[column] = le\n"
      ],
      "metadata": {
        "id": "rAr1-O5e2qz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tt = train_data.drop(['PassengerId', 'Survived'], axis=1)\n",
        "y_tt = train_data['Survived']\n",
        "\n"
      ],
      "metadata": {
        "id": "h9iJay_noITA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Normalize the data\n",
        "# scaler = StandardScaler()\n",
        "# X = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "QmeQrRLHm0Ez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to numpy arrays...ONCE possible only\n",
        "X= X_tt.values\n",
        "y= y_tt.values"
      ],
      "metadata": {
        "id": "kbR20pxYpwAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "6equ_q1ynOkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_tt))\n",
        "print(type(X))\n",
        "print(type(X_train))"
      ],
      "metadata": {
        "id": "hUdXWz-5rx_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Convert to PyTorch tensors\n",
        "# Convert X features to float tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "X_test_tensor = torch.FloatTensor(X_test)\n",
        "\n",
        "# Convert y labels to long tensors\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Verify lengths\n",
        "len(X_train_tensor), len(X_test_tensor), len(y_train_tensor), len(y_test_tensor)"
      ],
      "metadata": {
        "id": "5lhRO0K5qstv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loaded into tensor model.. otherwise u cant use tensor functions\n",
        "# Create TensorDataset objects\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "len(train_loader), len(test_loader)"
      ],
      "metadata": {
        "id": "ahkcEPeHoQLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TitanicModel, self).__init__()\n",
        "        self.layer_1 = nn.Linear(X_train_tensor.shape[1], 128)\n",
        "        self.layer_2 = nn.Linear(128, 64)\n",
        "        self.layer_3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()  # using ReLU activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer_1(x))\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.layer_3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jxiGMLP6_k6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TitanicModel()\n",
        "criterion = nn.BCEWithLogitsLoss()      # loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # using Adam as optimizer and learning rate\n"
      ],
      "metadata": {
        "id": "ilyZN0OnzFsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of epochs\n",
        "epochs = 200\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Forward pass\n",
        "        y_pred = model(batch_X).squeeze()  # Get predicted results and remove extra dimensions\n",
        "\n",
        "        # Measure the loss/error\n",
        "        loss = criterion(y_pred, batch_y.float())  # Convert batch_y to float for BCEWithLogitsLoss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Average loss for this epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "    # Print every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch: {epoch}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "# The loss values are stored in the 'losses' list\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cYmkQ-cv0Jmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model and compare predictions with known y_test values\n",
        "correct = 0\n",
        "total = 0\n",
        "# Initialize lists to store losses\n",
        "\n",
        "# Ensure the model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        # Forward pass\n",
        "        y_pred = model(batch_X).squeeze()  # Get predicted results and remove extra dimensions\n",
        "\n",
        "        # Apply sigmoid to convert logits to probabilities\n",
        "        y_pred_prob = torch.sigmoid(y_pred)\n",
        "\n",
        "        # Round to get the predicted class (0 or 1)\n",
        "        predicted_classes = torch.round(y_pred_prob)\n",
        "\n",
        "        # Compare with\n",
        "        # Compare with actual labels\n",
        "        correct += (predicted_classes.squeeze() == batch_y).sum().item()\n",
        "        total += batch_y.size(0)  # Accumulate the number of samples processed\n",
        "\n",
        "print(f'We got {correct} correct out of {total}!')\n",
        "print(f'Accuracy: {correct / total:.4f}')\n",
        "\n",
        "#we can use learning graphs for better know\n"
      ],
      "metadata": {
        "id": "fTWe2-Cb_4dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "\n",
        "test_data = test_data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Cabin'], axis=1)\n",
        "test_data['Age'] = imputer.transform(test_data[['Age']])\n",
        "\n",
        "for column in ['Sex', 'Embarked']:\n",
        "    test_data[column] = label_encoders[column].transform(test_data[column])\n",
        "\n"
      ],
      "metadata": {
        "id": "_YgCdcw9_zNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dont remove rows from testdata\n",
        "test_data.isnull().sum()"
      ],
      "metadata": {
        "id": "9a6gjS-ZAIBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "id": "fUoFWiz4BHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "LmUOxKGpATiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the test data for prediction\n",
        "# Drop the 'PassengerId' column as it's not a feature\n",
        "X_test = test_data.drop(['PassengerId'], axis=1)\n",
        "\n",
        "# Save the 'PassengerId' column to use later for the output file\n",
        "PassengerId = test_data['PassengerId']\n",
        "\n",
        "# Apply the same scaler used on the training data to the test data\n",
        "# X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert the scaled test data to a PyTorch tensor\n",
        "X_test_tensor = torch.FloatTensor(X_test.values)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for prediction\n",
        "with torch.no_grad():\n",
        "    # Forward pass: compute the model output for the test data\n",
        "    outputs = model(X_test_tensor).squeeze()\n",
        "\n",
        "    # Convert logits to probabilities and then to binary predictions\n",
        "    predictions = torch.round(torch.sigmoid(outputs)).long()\n",
        "\n"
      ],
      "metadata": {
        "id": "bTTBd3b7AGJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame to store the 'PassengerId' and the corresponding predictions\n",
        "output_df = pd.DataFrame({\n",
        "    'PassengerId': PassengerId,\n",
        "    'Survived': predictions.numpy()\n",
        "})\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/output.csv', index=False)\n",
        "\n",
        "# Print a message indicating that the output has been saved\n",
        "print('Output saved to output.csv')\n"
      ],
      "metadata": {
        "id": "2y6goJUsU_Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tgPL_xY6QQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPAVFUA5YcTAwsvUv7hSkua",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}