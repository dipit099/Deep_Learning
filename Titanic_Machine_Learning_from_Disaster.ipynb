{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipit099/Deep_Learning-Colab/blob/main/Titanic_Machine_Learning_from_Disaster.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GiJNidoA5VU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eay_k4LEBbfe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data  = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/train.csv')"
      ],
      "metadata": {
        "id": "F_fkmunpCtV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "id": "GciqgctiCt2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "vw8DptstCwp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "id": "KxV7b3z8Cxt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.columns"
      ],
      "metadata": {
        "id": "w2sDzf0uC4F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "object_columns = train_data.select_dtypes(include=['object']).columns\n",
        "print(object_columns)"
      ],
      "metadata": {
        "id": "4nr8tk3nDJh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "metadata": {
        "id": "OrIZhAT_CzcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "itGbJZDRInUw"
      },
      "outputs": [],
      "source": [
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create a scatter plot\n",
        "sns.scatterplot(x='Fare', y='Survived', data=train_data, hue='Survived', palette={0: 'red', 1: 'green'})\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title('Fare vs Survived (Scatter Plot)')\n",
        "plt.xlabel('Fare')\n",
        "plt.ylabel('Survived (0 = No, 1 = Yes)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading and Preprocessing:\n",
        "train_data = train_data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Cabin'], axis=1)\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "train_data['Age'] = imputer.fit_transform(train_data[['Age']])\n",
        "\n",
        "label_encoders = {}\n",
        "for column in ['Sex', 'Embarked']:\n",
        "    le = LabelEncoder()\n",
        "    train_data[column] = le.fit_transform(train_data[column])\n",
        "    label_encoders[column] = le\n",
        "\n",
        "X = train_data.drop(['PassengerId', 'Survived'], axis=1)\n",
        "y = train_data['Survived']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.int64)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "rAr1-O5e2qz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition\n",
        "class TitanicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TitanicModel, self).__init__()\n",
        "        self.layer_1 = nn.Linear(X_tensor.shape[1], 128)\n",
        "        self.layer_2 = nn.Linear(128, 64)\n",
        "        self.layer_3 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer_1(x))\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.layer_3(x)\n",
        "        return x\n",
        "\n",
        "model = TitanicModel()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "jxiGMLP6_k6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Data\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X).squeeze()\n",
        "        loss = criterion(outputs, batch_y.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item() * batch_X.size(0)\n",
        "\n",
        "    epoch_loss /= len(train_loader.dataset)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "4weGY86v_xks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on testdata and compare\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X).squeeze()\n",
        "            loss = criterion(outputs, batch_y.float())\n",
        "            total_loss += loss.item() * batch_X.size(0)\n",
        "            predictions = torch.round(torch.sigmoid(outputs))\n",
        "            correct += (predictions == batch_y).sum().item()\n",
        "            total += batch_y.size(0)\n",
        "\n",
        "    average_test_loss = total_loss / len(test_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f'Average Test Loss: {average_test_loss:.4f}')\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "evaluate_model(model, test_loader, criterion)\n"
      ],
      "metadata": {
        "id": "fTWe2-Cb_4dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/test.csv')\n",
        "test_data = test_data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Cabin'], axis=1)\n",
        "\n",
        "test_data['Age'] = imputer.transform(test_data[['Age']])\n",
        "\n",
        "for column in ['Sex', 'Embarked']:\n",
        "    test_data[column] = label_encoders[column].transform(test_data[column])\n",
        "\n"
      ],
      "metadata": {
        "id": "_YgCdcw9_zNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dont remove rows from testdata\n",
        "test_data.isnull().sum()"
      ],
      "metadata": {
        "id": "9a6gjS-ZAIBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "id": "fUoFWiz4BHdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "LmUOxKGpATiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_test = test_data.drop(['PassengerId'], axis=1)\n",
        "PassengerId = test_data['PassengerId']\n",
        "\n",
        "X_test = scaler.transform(X_test)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_tensor).squeeze()\n",
        "    predictions = torch.round(torch.sigmoid(outputs)).long()\n",
        "\n",
        "output_df = pd.DataFrame({\n",
        "    'PassengerId': PassengerId,\n",
        "    'Survived': predictions.numpy()\n",
        "})\n",
        "\n",
        "output_df.to_csv('/content/drive/MyDrive/Colab Notebooks/datasets/titanic/output.csv', index=False)\n",
        "print('Output saved to output.csv')"
      ],
      "metadata": {
        "id": "bTTBd3b7AGJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4w6E8TpASOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOE9YXpCvVfg54QqxyUhQ4M",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}