{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e5df4c",
   "metadata": {
    "papermill": {
     "duration": 0.00696,
     "end_time": "2024-07-27T05:34:07.484763",
     "exception": false,
     "start_time": "2024-07-27T05:34:07.477803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Intro\n",
    "\n",
    "Data comes in many different forms: time stamps, sensor readings, images, categorical labels, and so much more. But text is still some of the most valuable data out there for those who know how to use it.  \n",
    "\n",
    "In this course about **Natural Language Processing (NLP)**, you will use the leading NLP library (spaCy) to take on some of the most important tasks in working with text. \n",
    "\n",
    "By the end, you will be able to use spaCy for:\n",
    "\n",
    "* Basic text processing and pattern matching\n",
    "* Building machine learning models with text\n",
    "* Representing text with word embeddings that numerically capture the meaning of words and documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440df8d",
   "metadata": {
    "papermill": {
     "duration": 0.006167,
     "end_time": "2024-07-27T05:34:07.497748",
     "exception": false,
     "start_time": "2024-07-27T05:34:07.491581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To get the most out of this course, you'll need some experience with machine learning. If you don't have experience with scikit-learn, check out [Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning) and [Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning) to learn the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec66df",
   "metadata": {
    "papermill": {
     "duration": 0.006363,
     "end_time": "2024-07-27T05:34:07.510669",
     "exception": false,
     "start_time": "2024-07-27T05:34:07.504306",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## NLP with spaCy\n",
    "\n",
    "spaCy is the leading library for NLP, and it has quickly become one of the most popular Python frameworks. Most people find it intuitive, and it has excellent [documentation](https://spacy.io/usage).\n",
    "\n",
    "spaCy relies on **models** that are language-specific and come in different sizes. You can load a spaCy model with `spacy.load`. \n",
    "\n",
    "For example, here's how you would load the English language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd98ed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T05:34:07.525451Z",
     "iopub.status.busy": "2024-07-27T05:34:07.524989Z",
     "iopub.status.idle": "2024-07-27T05:34:14.827251Z",
     "shell.execute_reply": "2024-07-27T05:34:14.825939Z"
    },
    "papermill": {
     "duration": 7.312917,
     "end_time": "2024-07-27T05:34:14.830028",
     "exception": false,
     "start_time": "2024-07-27T05:34:07.517111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# SO here nlp is an object loading English model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607022da",
   "metadata": {
    "papermill": {
     "duration": 0.006243,
     "end_time": "2024-07-27T05:34:14.843140",
     "exception": false,
     "start_time": "2024-07-27T05:34:14.836897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With the model loaded, you can process text like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0481c455",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T05:34:14.858335Z",
     "iopub.status.busy": "2024-07-27T05:34:14.857686Z",
     "iopub.status.idle": "2024-07-27T05:34:14.891495Z",
     "shell.execute_reply": "2024-07-27T05:34:14.890153Z"
    },
    "papermill": {
     "duration": 0.044517,
     "end_time": "2024-07-27T05:34:14.894203",
     "exception": false,
     "start_time": "2024-07-27T05:34:14.849686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea is healthy and calming, don't you think? Hello I am Dipit!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tea is healthy and calming, don't you think? Hello I am Dipit!\")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f59a7",
   "metadata": {
    "papermill": {
     "duration": 0.006268,
     "end_time": "2024-07-27T05:34:14.907312",
     "exception": false,
     "start_time": "2024-07-27T05:34:14.901044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There's a lot you can do with the `doc` object you just created.\n",
    "\n",
    "# Tokenizing\n",
    "\n",
    "This returns a document object that contains **tokens**. A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractions like \"don't\" into two tokens, \"do\" and \"n't\". You can see the tokens by iterating through the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737f6718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T05:34:14.922136Z",
     "iopub.status.busy": "2024-07-27T05:34:14.921712Z",
     "iopub.status.idle": "2024-07-27T05:34:14.927927Z",
     "shell.execute_reply": "2024-07-27T05:34:14.926724Z"
    },
    "papermill": {
     "duration": 0.016578,
     "end_time": "2024-07-27T05:34:14.930380",
     "exception": false,
     "start_time": "2024-07-27T05:34:14.913802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "calming\n",
      ",\n",
      "do\n",
      "n't\n",
      "you\n",
      "think\n",
      "?\n",
      "Hello\n",
      "I\n",
      "am\n",
      "Dipit\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bc353",
   "metadata": {
    "papermill": {
     "duration": 0.006502,
     "end_time": "2024-07-27T05:34:14.943742",
     "exception": false,
     "start_time": "2024-07-27T05:34:14.937240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Iterating through a document gives you token objects. Each of these tokens comes with additional information. In most cases, the important ones are `token.lemma_` and `token.is_stop`.\n",
    "\n",
    "# Text preprocessing\n",
    "\n",
    "There are a few types of preprocessing to improve how we model with words. The first is \"lemmatizing.\"\n",
    "The \"lemma\" of a word is its base form.  For example, \"walk\" is the lemma of the word \"walking\". So, when you lemmatize the word walking, you would convert it to walk.\n",
    "\n",
    "It's also common to remove stopwords. Stopwords are words that occur frequently in the language and don't contain much information. English  stopwords include \"the\", \"is\", \"and\", \"but\", \"not\". \n",
    "\n",
    "With a spaCy token, `token.lemma_` returns the lemma, while `token.is_stop` returns a boolean `True` if the token is a stopword (and `False` otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c9ed7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T05:34:14.958964Z",
     "iopub.status.busy": "2024-07-27T05:34:14.958539Z",
     "iopub.status.idle": "2024-07-27T05:34:14.965875Z",
     "shell.execute_reply": "2024-07-27T05:34:14.964571Z"
    },
    "papermill": {
     "duration": 0.018515,
     "end_time": "2024-07-27T05:34:14.969076",
     "exception": false,
     "start_time": "2024-07-27T05:34:14.950561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "calming\t\tcalming\t\tFalse\n",
      ",\t\t,\t\tFalse\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tnot\t\tTrue\n",
      "you\t\tyou\t\tTrue\n",
      "think\t\tthink\t\tFalse\n",
      "?\t\t?\t\tFalse\n",
      "Hello\t\thello\t\tFalse\n",
      "I\t\tI\t\tTrue\n",
      "am\t\tbe\t\tTrue\n",
      "Dipit\t\tDipit\t\tFalse\n",
      "!\t\t!\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "# stop words are those which dont occur frequently in a sentence\n",
    "# The first is \"lemmatizing.\" The \"lemma\" of a word is its base form. For example, \"walk\" is the lemma of the word \"walking\". \n",
    "print(\"Token \\t\\tLemma \\t\\tStopword\")\n",
    "\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436678a",
   "metadata": {
    "papermill": {
     "duration": 0.00668,
     "end_time": "2024-07-27T05:34:14.982750",
     "exception": false,
     "start_time": "2024-07-27T05:34:14.976070",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Why are lemmas and identifying stopwords important? Language data has a lot of noise mixed in with informative content. In the sentence above, the important words are tea, healthy and calming. Removing stop words might help the predictive model focus on relevant words. Lemmatizing similarly helps by combining multiple forms of the same word into one base form (\"calming\", \"calms\", \"calmed\" would all change to \"calm\").\n",
    "\n",
    "However, lemmatizing and dropping stopwords might result in your models performing worse. So you should treat this preprocessing as part of your hyperparameter optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ddc6b",
   "metadata": {
    "papermill": {
     "duration": 0.006464,
     "end_time": "2024-07-27T05:34:14.996120",
     "exception": false,
     "start_time": "2024-07-27T05:34:14.989656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pattern Matching\n",
    "\n",
    "Another common NLP task is matching tokens or phrases within chunks of text or whole documents. You can do pattern matching with regular expressions, but spaCy's matching capabilities tend to be easier to use.\n",
    "\n",
    "To match individual tokens, you create a `Matcher`. When you want to match a list of terms, it's easier and more efficient to use `PhraseMatcher`. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest. First you create the `PhraseMatcher` itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1c8eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T05:34:15.012151Z",
     "iopub.status.busy": "2024-07-27T05:34:15.011763Z",
     "iopub.status.idle": "2024-07-27T05:34:15.018290Z",
     "shell.execute_reply": "2024-07-27T05:34:15.017218Z"
    },
    "papermill": {
     "duration": 0.018472,
     "end_time": "2024-07-27T05:34:15.021676",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.003204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created!\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "# note: nlp is the object that i have loaded in 1st code !\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "print(\"model created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da3e0bd",
   "metadata": {
    "papermill": {
     "duration": 0.006681,
     "end_time": "2024-07-27T05:34:15.035309",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.028628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting `attr='LOWER'` will match the phrases on lowercased text. This provides case insensitive matching.\n",
    "\n",
    "Next you create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the `nlp` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cffc3a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T05:34:15.051108Z",
     "iopub.status.busy": "2024-07-27T05:34:15.050695Z",
     "iopub.status.idle": "2024-07-27T05:34:15.088132Z",
     "shell.execute_reply": "2024-07-27T05:34:15.086915Z"
    },
    "papermill": {
     "duration": 0.048429,
     "end_time": "2024-07-27T05:34:15.090816",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.042387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patterns obj added to your loaded matcher model\n"
     ]
    }
   ],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "# This line adds the created patterns to the PhraseMatcher instance called matcher.\n",
    "#\"TerminologyList\": This is the name or label assigned to this group of patterns. \n",
    "#It's a string identifier used to categorize the match results.\n",
    "print(\"patterns obj added to your loaded matcher model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d911bc",
   "metadata": {
    "papermill": {
     "duration": 0.006832,
     "end_time": "2024-07-27T05:34:15.105035",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.098203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then you create a document from the text to search and use the phrase matcher to find where the terms occur in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42afd0ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T05:34:15.121229Z",
     "iopub.status.busy": "2024-07-27T05:34:15.120477Z",
     "iopub.status.idle": "2024-07-27T05:34:15.144879Z",
     "shell.execute_reply": "2024-07-27T05:34:15.143511Z"
    },
    "papermill": {
     "duration": 0.035296,
     "end_time": "2024-07-27T05:34:15.147453",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.112157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
     ]
    }
   ],
   "source": [
    "# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\n",
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last year’s iPhone XS and Google Pixel 3.\") \n",
    "matches = matcher(text_doc)\n",
    "print(matches)\n",
    "#The match ID is a unique identifier genera..using a hash function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27fd34f",
   "metadata": {
    "papermill": {
     "duration": 0.006791,
     "end_time": "2024-07-27T05:34:15.161447",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.154656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The matches here are a tuple of the match id and the positions of the start and end of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55c01611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T05:34:15.178260Z",
     "iopub.status.busy": "2024-07-27T05:34:15.177851Z",
     "iopub.status.idle": "2024-07-27T05:34:15.184999Z",
     "shell.execute_reply": "2024-07-27T05:34:15.183751Z"
    },
    "papermill": {
     "duration": 0.018631,
     "end_time": "2024-07-27T05:34:15.188067",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.169436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern Label: TerminologyList, Matched Phrase: iPhone 11, Start: 17, End: 19\n",
      "Pattern Label: TerminologyList, Matched Phrase: Galaxy Note, Start: 22, End: 24\n",
      "Pattern Label: TerminologyList, Matched Phrase: iPhone XS, Start: 30, End: 32\n",
      "Pattern Label: TerminologyList, Matched Phrase: Google Pixel, Start: 33, End: 35\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all matches and print their details\n",
    "for match_id, start, end in matches:\n",
    "    # Retrieve the label associated with the match ID\n",
    "    #nlp.vocab.strings helps to extract the id only from hash code match_id i guess!\n",
    "    \n",
    "    pattern_label = nlp.vocab.strings[match_id]\n",
    "    \n",
    "    # Extract the matched span from the text document! MAIN THING !!\n",
    "    matched_span = text_doc[start:end]\n",
    "    \n",
    "    # Print the pattern label and the matched text span\n",
    "    print(f\"Pattern Label: {pattern_label}, Matched Phrase: {matched_span.text}, Start: {start}, End: {end}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201da4a",
   "metadata": {
    "papermill": {
     "duration": 0.006903,
     "end_time": "2024-07-27T05:34:15.202331",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.195428",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Your Turn\n",
    "Now that you've seen a few uses of SpaCy for NLP, it's your turn to try it to **[analyze Yelp reviews](https://www.kaggle.com/kernels/fork/6061023)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d901f",
   "metadata": {
    "papermill": {
     "duration": 0.006874,
     "end_time": "2024-07-27T05:34:15.216472",
     "exception": false,
     "start_time": "2024-07-27T05:34:15.209598",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/natural-language-processing/discussion) to chat with other learners.*"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 348259,
     "sourceId": 695175,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 362178,
     "sourceId": 763778,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.946862,
   "end_time": "2024-07-27T05:34:16.348185",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-27T05:34:04.401323",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
